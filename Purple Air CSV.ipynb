{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "treated-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-young",
   "metadata": {},
   "source": [
    "# Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broke-underwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory /Users/jenniferbadger/Documents/GitHub/229project\n",
      "Current working directory /Users/jenniferbadger/Documents/GitHub/229project/PAprimary\n"
     ]
    }
   ],
   "source": [
    "#from https://www.tutorialspoint.com/python/os_chdir.htm\n",
    "#first retrieving working directory\n",
    "#retval = os.getcwd()\n",
    "#print (\"Current working directory %s\" % retval)\n",
    "#returned: Current working directory /Users/jenniferbadger/Documents/GitHub/229project\n",
    "\n",
    "#now changing working directory to where CSVs are located\n",
    "#os.chdir(\"/Users/jenniferbadger/Documents/GitHub/229project/PAprimary\")\n",
    "#retval = os.getcwd()\n",
    "#print (\"Current working directory %s\" % retval)\n",
    "#returned: Current working directory /Users/jenniferbadger/Documents/GitHub/229project/PAprimary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-driving",
   "metadata": {},
   "source": [
    "# CSV Wrangling\n",
    "\n",
    "These CSVs were exported from Purple Air's data download tool and contain PM 2.5 24 hour average data for 2019. \n",
    "\n",
    "The sensors were chosen using the Purple Air interactive map to subset first on a portion of greater LA that would include most of the HOLC boundaries and on sensors with names A-C that began delivering data during or before 2019. The reason for subsetting is that I ran into issues with hardware memory, limited interaction capability with regard to which CSVs I could choose (I had to download primary and secondary datasets for each sensor although the secorndary datasets were useless) and considering historical data was unavailable via the API. Names are defined by users and are not associated with location so are spatially random. Importantly, this subset yielded California Clean Air Coalition and AQMD/NASA sensors which are well distributed; eg. Echo Park, Pacoima, Brentwood, Inglewood, Montebello, Arcadia, Pacific Palisades to name a few. \n",
    "\n",
    "This subset yielded observations for some or all of the days in 2019 for 116 different point locations throughout greater Los Angeles.\n",
    "\n",
    "https://api.purpleair.com\n",
    "https://www.purpleair.com/sensorlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interracial-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are all of the filenames ending in .csv ['Brentwood School West Campus (outside) (34.061087 -118.47584) Primary 1440_minute_average 01_01_2019 12_31_2019.csv'].\n"
     ]
    }
   ],
   "source": [
    "#from https://sempioneer.com/python-for-seo/how-to-combine-multiple-csv-files-in-python/\n",
    "#file_extension = '.csv'\n",
    "#all_filenames = [i for i in glob.glob(f\"*{file_extension}\")]\n",
    "\n",
    "#checking correct files are retieved\n",
    "#print(f\"These are all of the filenames ending in .csv {all_filenames}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "immediate-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='Brentwood School West Campus (outside) (34.061087 -118.47584) Primary 1440_minute_average 01_01_2019 12_31_2019.csv' mode='r' encoding='UTF-8'>\n",
      "(248, 10)\n"
     ]
    }
   ],
   "source": [
    "# prelim to concatenating two to see if we lose row 0 which contains location information\n",
    "#all_filenames[0]\n",
    "\n",
    "#cheking encoding of PA CSVs\n",
    "#with open('Brentwood School West Campus (outside) (34.061087 -118.47584) Primary 1440_minute_average 01_01_2019 12_31_2019.csv') as q:\n",
    "    #print(q)\n",
    "\n",
    "#adapting code with encoding='UTF-8'    \n",
    "#df = pd.read_csv('Brentwood School West Campus (outside) (34.061087 -118.47584) Primary 1440_minute_average 01_01_2019 12_31_2019.csv', \n",
    "                 #delimiter=',', encoding='UTF-8')\n",
    "\n",
    "#print(df.shape) # shape is correct! but missing important location info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naked-bronze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>PM1.0_CF1_ug/m3</th>\n",
       "      <th>PM2.5_CF1_ug/m3</th>\n",
       "      <th>PM10.0_CF1_ug/m3</th>\n",
       "      <th>UptimeMinutes</th>\n",
       "      <th>RSSI_dbm</th>\n",
       "      <th>Temperature_F</th>\n",
       "      <th>Humidity_%</th>\n",
       "      <th>PM2.5_ATM_ug/m3</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-01 00:00:00 UTC</td>\n",
       "      <td>10.86</td>\n",
       "      <td>34.12</td>\n",
       "      <td>53.79</td>\n",
       "      <td>50.66</td>\n",
       "      <td>-53.56</td>\n",
       "      <td>84.3</td>\n",
       "      <td>27.37</td>\n",
       "      <td>23.32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-01 00:00:00 UTC</td>\n",
       "      <td>10.86</td>\n",
       "      <td>34.12</td>\n",
       "      <td>53.79</td>\n",
       "      <td>50.66</td>\n",
       "      <td>-53.56</td>\n",
       "      <td>84.3</td>\n",
       "      <td>27.37</td>\n",
       "      <td>23.32</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-02 00:00:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-02 00:00:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-03 00:00:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2019-11-07 00:00:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2019-11-08 00:00:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2019-11-09 00:00:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2019-11-10 00:00:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2019-11-11 00:00:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at  PM1.0_CF1_ug/m3  PM2.5_CF1_ug/m3  \\\n",
       "0    2019-08-01 00:00:00 UTC            10.86            34.12   \n",
       "1    2019-08-01 00:00:00 UTC            10.86            34.12   \n",
       "2    2019-08-02 00:00:00 UTC              NaN              NaN   \n",
       "3    2019-08-02 00:00:00 UTC              NaN              NaN   \n",
       "4    2019-08-03 00:00:00 UTC              NaN              NaN   \n",
       "..                       ...              ...              ...   \n",
       "193  2019-11-07 00:00:00 UTC              NaN              NaN   \n",
       "194  2019-11-08 00:00:00 UTC              NaN              NaN   \n",
       "195  2019-11-09 00:00:00 UTC              NaN              NaN   \n",
       "196  2019-11-10 00:00:00 UTC              NaN              NaN   \n",
       "197  2019-11-11 00:00:00 UTC              NaN              NaN   \n",
       "\n",
       "     PM10.0_CF1_ug/m3  UptimeMinutes  RSSI_dbm  Temperature_F  Humidity_%  \\\n",
       "0               53.79          50.66    -53.56           84.3       27.37   \n",
       "1               53.79          50.66    -53.56           84.3       27.37   \n",
       "2                 NaN            NaN       NaN            NaN         NaN   \n",
       "3                 NaN            NaN       NaN            NaN         NaN   \n",
       "4                 NaN            NaN       NaN            NaN         NaN   \n",
       "..                ...            ...       ...            ...         ...   \n",
       "193               NaN            NaN       NaN            NaN         NaN   \n",
       "194               NaN            NaN       NaN            NaN         NaN   \n",
       "195               NaN            NaN       NaN            NaN         NaN   \n",
       "196               NaN            NaN       NaN            NaN         NaN   \n",
       "197               NaN            NaN       NaN            NaN         NaN   \n",
       "\n",
       "     PM2.5_ATM_ug/m3  Unnamed: 9  \n",
       "0              23.32         NaN  \n",
       "1              23.32         NaN  \n",
       "2                NaN         NaN  \n",
       "3                NaN         NaN  \n",
       "4                NaN         NaN  \n",
       "..               ...         ...  \n",
       "193              NaN         NaN  \n",
       "194              NaN         NaN  \n",
       "195              NaN         NaN  \n",
       "196              NaN         NaN  \n",
       "197              NaN         NaN  \n",
       "\n",
       "[198 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing concatenating three csvs\n",
    "#combined_csv_data = pd.concat([pd.read_csv(f, delimiter=',', encoding='UTF-8') for f in all_filenames])\n",
    "\n",
    "#combined_csv_data.head(-50) #concatenates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simple-money",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>filename</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>PM2.5 ATM Mean</th>\n",
       "      <th>PM2.5 CF1 Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-16 00:00:00 UTC</td>\n",
       "      <td>CCA 61st and King (outside) (33.98329 -118.183...</td>\n",
       "      <td>33.98329</td>\n",
       "      <td>-118.183926</td>\n",
       "      <td>16.995496</td>\n",
       "      <td>18.172958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-16 00:00:00 UTC</td>\n",
       "      <td>CCA 52nd Dr and Heliotrope (outside) (33.98995...</td>\n",
       "      <td>33.98995</td>\n",
       "      <td>-118.178093</td>\n",
       "      <td>16.328882</td>\n",
       "      <td>17.597437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-26 00:00:00 UTC</td>\n",
       "      <td>Adams Square (inside) (inside) (34.133594 -118...</td>\n",
       "      <td>34.133594</td>\n",
       "      <td>-118.241213</td>\n",
       "      <td>12.812237</td>\n",
       "      <td>14.105855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-27 00:00:00 UTC</td>\n",
       "      <td>CSUN Live Oak (outside) (34.238179 -118.528333...</td>\n",
       "      <td>34.238179</td>\n",
       "      <td>-118.528333</td>\n",
       "      <td>13.484826</td>\n",
       "      <td>13.641279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-09 00:00:00 UTC</td>\n",
       "      <td>6th St (inside) (34.020076 -118.49534) Primary...</td>\n",
       "      <td>34.020076</td>\n",
       "      <td>-118.49534</td>\n",
       "      <td>6.793395</td>\n",
       "      <td>7.069488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2019-04-10 00:00:00 UTC</td>\n",
       "      <td>CCA 56th and Atlantic (outside) (33.990066 -11...</td>\n",
       "      <td>33.990066</td>\n",
       "      <td>-118.185125</td>\n",
       "      <td>17.797489</td>\n",
       "      <td>19.23635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2019-05-06 00:00:00 UTC</td>\n",
       "      <td>COI5467 (outside) (34.085285 -117.960899) Prim...</td>\n",
       "      <td>34.085285</td>\n",
       "      <td>-117.960899</td>\n",
       "      <td>29.841667</td>\n",
       "      <td>37.285972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2019-01-15 00:00:00 UTC</td>\n",
       "      <td>10 (outside) (34.167005 -118.03768) Primary 14...</td>\n",
       "      <td>34.167005</td>\n",
       "      <td>-118.03768</td>\n",
       "      <td>54.275</td>\n",
       "      <td>79.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2019-01-01 00:00:00 UTC</td>\n",
       "      <td>Bel Air (outside) (34.094154 -118.462079) Prim...</td>\n",
       "      <td>34.094154</td>\n",
       "      <td>-118.462079</td>\n",
       "      <td>11.429668</td>\n",
       "      <td>11.857695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2019-11-21 00:00:00 UTC</td>\n",
       "      <td>Butterfield (outside) (34.03872 -118.416681) P...</td>\n",
       "      <td>34.03872</td>\n",
       "      <td>-118.416681</td>\n",
       "      <td>22.035</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  \\\n",
       "0   2019-03-16 00:00:00 UTC   \n",
       "1   2019-03-16 00:00:00 UTC   \n",
       "2   2019-03-26 00:00:00 UTC   \n",
       "3   2019-03-27 00:00:00 UTC   \n",
       "4   2019-01-09 00:00:00 UTC   \n",
       "..                      ...   \n",
       "95  2019-04-10 00:00:00 UTC   \n",
       "96  2019-05-06 00:00:00 UTC   \n",
       "97  2019-01-15 00:00:00 UTC   \n",
       "98  2019-01-01 00:00:00 UTC   \n",
       "99  2019-11-21 00:00:00 UTC   \n",
       "\n",
       "                                             filename        lat        long  \\\n",
       "0   CCA 61st and King (outside) (33.98329 -118.183...   33.98329 -118.183926   \n",
       "1   CCA 52nd Dr and Heliotrope (outside) (33.98995...   33.98995 -118.178093   \n",
       "2   Adams Square (inside) (inside) (34.133594 -118...  34.133594 -118.241213   \n",
       "3   CSUN Live Oak (outside) (34.238179 -118.528333...  34.238179 -118.528333   \n",
       "4   6th St (inside) (34.020076 -118.49534) Primary...  34.020076  -118.49534   \n",
       "..                                                ...        ...         ...   \n",
       "95  CCA 56th and Atlantic (outside) (33.990066 -11...  33.990066 -118.185125   \n",
       "96  COI5467 (outside) (34.085285 -117.960899) Prim...  34.085285 -117.960899   \n",
       "97  10 (outside) (34.167005 -118.03768) Primary 14...  34.167005  -118.03768   \n",
       "98  Bel Air (outside) (34.094154 -118.462079) Prim...  34.094154 -118.462079   \n",
       "99  Butterfield (outside) (34.03872 -118.416681) P...   34.03872 -118.416681   \n",
       "\n",
       "   PM2.5 ATM Mean PM2.5 CF1 Mean  \n",
       "0       16.995496      18.172958  \n",
       "1       16.328882      17.597437  \n",
       "2       12.812237      14.105855  \n",
       "3       13.484826      13.641279  \n",
       "4        6.793395       7.069488  \n",
       "..            ...            ...  \n",
       "95      17.797489       19.23635  \n",
       "96      29.841667      37.285972  \n",
       "97         54.275         79.555  \n",
       "98      11.429668      11.857695  \n",
       "99         22.035           32.2  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "path = r'/Users/jenniferbadger/Documents/GitHub/229project/PAprimary' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "\n",
    "li = [] #only need this if appending to list instead of df\n",
    "#https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\n",
    "\n",
    "for filename in all_files:\n",
    "    #reads csv into a dataframe\n",
    "    df = pd.read_csv(filename, index_col=None,encoding='UTF-8', delimiter=',')\n",
    "    #print(filename)\n",
    "    filename = filename.split('/')\n",
    "    filename = filename[-1]\n",
    "    #print(filename)\n",
    "    #adds csv extention- may be unnecessary\n",
    "    #file_extension = '.csv'\n",
    "    #converts file name to a i string list\n",
    "    #filename = glob.glob(f\"*{file_extension}\")\n",
    "    #adds file name to column\n",
    "    df['filename'] = filename\n",
    "    #print(df['filename'])\n",
    "    #creates a list of parsed out strings inside parenthesis\n",
    "    parenth = re.findall('\\(([^)]+)', filename)\n",
    "    #creates a 2 string list of lat, long\n",
    "    lat_long = parenth[-1].split(\" \")\n",
    "    #extracts lat/long from list assignes them to column\n",
    "    #and coverts them to float\n",
    "    df['lat'] = lat_long[0] #astype(float)\n",
    "    df['lat'] = df['lat'].astype(float)\n",
    "    df['long'] = lat_long[-1] \n",
    "    df['long'] = df['long'].astype(float)\n",
    "    #replaces NaN with zeroes\n",
    "    df['PM2.5_ATM_ug/m3']= df['PM2.5_ATM_ug/m3'].fillna(0)\n",
    "    df['PM2.5_CF1_ug/m3'] = df['PM2.5_CF1_ug/m3'].fillna(0)\n",
    "    #filters for rows where Pm 2.5 values don't equal zero\n",
    "    df = df[(df['PM2.5_ATM_ug/m3'] != 0)]\n",
    "    #df = df[(df['PM2.5_CF1_ug/m3'] != 0)]\n",
    "    #takes mean pm2.5\n",
    "    df['PM2.5 ATM Mean'] = df['PM2.5_ATM_ug/m3'].mean()\n",
    "    df['PM2.5 CF1 Mean'] = df['PM2.5_CF1_ug/m3'].mean()\n",
    "    #df['PM2.5 ATM Mean'] = df[(df['PM2.5_ATM_ug/m3'] != 0)].mean()\n",
    "    #df['PM2.5 CF1 Mean'] = df[(df['PM2.5_CF1_ug/m3']!= 0)].mean()\n",
    "    \n",
    "    #PM25_ATM_Mean = df[(df['PM2.5_ATM_ug/m3'] != 0)].mean()\n",
    "    #PM25_CFI_Mean = df[(df['PM2.5_CF1_ug/m3']!= 0)].mean()\n",
    "    #appends to df\n",
    "    #df= df.append(df)\n",
    "    #alt way to append to list\n",
    "    \n",
    "    #I only need the values from row 0, columns 11-14 to concat into one csv\n",
    "    #drops unneeded columns\n",
    "    try:\n",
    "        df.drop(['PM1.0_CF1_ug/m3', 'PM2.5_CF1_ug/m3', 'PM10.0_CF1_ug/m3', 'UptimeMinutes', \n",
    "             'RSSI_dbm', 'Temperature_F', 'Humidity_%', 'PM2.5_ATM_ug/m3', 'Unnamed: 9'],\n",
    "            inplace=True, axis = 1) \n",
    "    except:\n",
    "        df.drop(['PM1.0_CF1_ug/m3', 'PM2.5_CF1_ug/m3', 'PM10.0_CF1_ug/m3', 'UptimeMinutes', \n",
    "             'RSSI_dbm', 'Temperature_F', 'Humidity_%', 'PM2.5_ATM_ug/m3'],\n",
    "            inplace=True, axis = 1) \n",
    "        continue\n",
    "    df = df.reset_index(drop=True)\n",
    "    #print(df)\n",
    "    try:\n",
    "        newdf = df.iloc[0]\n",
    "    except:\n",
    "        newdf = df.iloc[-1]\n",
    "        continue\n",
    "    #converting series to df and flipping on its end\n",
    "    newdf = pd.DataFrame(newdf).transpose()\n",
    "    #newdf['PM2.5 ATM Mean'] = PM25_ATM_Mean\n",
    "    #newdf['PM2.5 CF1 Mean'] = PM25_CFI_Mean\n",
    "    li.append(newdf)\n",
    "    #bigdf= bigdf.append(newdf)\n",
    "    #alt way to convert series to df\n",
    "    #newdf = newdf.to_frame()\n",
    "\n",
    "final_frame = pd.concat(li, axis=0, ignore_index=True) \n",
    "final_frame.head(-10)\n",
    "\n",
    "#newdf.head(70)\n",
    "#final_frame = pd.concat([newdf], axis=0, ignore_index=True) \n",
    "\n",
    "\n",
    "#some code for spllitting on parenthesis\n",
    "#s = 'Name(something)'\n",
    "#re.search('\\(([^)]+)', s).group(1)\n",
    "#'something'\n",
    "\n",
    "#https://stackoverflow.com/questions/45947887/python-looping-through-csv-files-and-their-columns\n",
    "#for i in numFiles:\n",
    "    #file = open(os.path.join(pathName, i), \"rU\")\n",
    "    #reader = csv.reader(file, delimiter=',')\n",
    "    #for row in reader:\n",
    "        #for column in row:\n",
    "            #print(column)\n",
    "            #if column==\"SPECIFIC VALUE\":\n",
    "                #do stuf                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "conventional-faculty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jenniferbadger/Documents/GitHub/229project/PAprimary/CSUN Live Oak (outside) (34.238179 -118.528333) Primary 1440_minute_average 01_01_2019 12_31_2019.csv'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-enough",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
